{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tokenize` --- Tokenizer for Python source\n",
    "\n",
    "tokenize\n",
    "\n",
    "Ka Ping Yee\n",
    "\n",
    "Fred L. Drake, Jr. \\<<fdrake@acm.org>\\>\n",
    "\n",
    "**Source code:** `Lib/tokenize.py`\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "The `tokenize` module provides a lexical scanner for Python source code,\n",
    "implemented in Python. The scanner in this module returns comments as\n",
    "tokens as well, making it useful for implementing \"pretty-printers\",\n",
    "including colorizers for on-screen displays.\n",
    "\n",
    "To simplify token stream handling, all `operator <operators>` and\n",
    "`delimiter <delimiters>` tokens and `Ellipsis` are returned using the\n",
    "generic `~token.OP` token type. The exact type can be determined by\n",
    "checking the `exact_type` property on the `named tuple` returned from\n",
    "`tokenize.tokenize`.\n",
    "\n",
    "## Tokenizing Input\n",
    "\n",
    "The primary entry point is a `generator`:\n",
    "\n",
    "tokenize(readline)\n",
    "\n",
    "The `.tokenize` generator requires one argument, *readline*, which must\n",
    "be a callable object which provides the same interface as the\n",
    "`io.IOBase.readline` method of file objects. Each call to the function\n",
    "should return one line of input as bytes.\n",
    "\n",
    "The generator produces 5-tuples with these members: the token type; the\n",
    "token string; a 2-tuple `(srow, scol)` of ints specifying the row and\n",
    "column where the token begins in the source; a 2-tuple `(erow, ecol)` of\n",
    "ints specifying the row and column where the token ends in the source;\n",
    "and the line on which the token was found. The line passed (the last\n",
    "tuple item) is the *physical* line. The 5 tuple is returned as a\n",
    "`named tuple` with the field names: `type string start end line`.\n",
    "\n",
    "The returned `named tuple` has an additional property named `exact_type`\n",
    "that contains the exact operator type for `~token.OP` tokens. For all\n",
    "other token types `exact_type` equals the named tuple `type` field.\n",
    "\n",
    "3.1 Added support for named tuples.\n",
    "\n",
    "3.3 Added support for `exact_type`.\n",
    "\n",
    "`.tokenize` determines the source encoding of the file by looking for a\n",
    "UTF-8 BOM or encoding cookie, according to `263`.\n",
    "\n",
    "generate_tokens(readline)\n",
    "\n",
    "Tokenize a source reading unicode strings instead of bytes.\n",
    "\n",
    "Like `.tokenize`, the *readline* argument is a callable returning a\n",
    "single line of input. However, `generate_tokens` expects *readline* to\n",
    "return a str object rather than bytes.\n",
    "\n",
    "The result is an iterator yielding named tuples, exactly like\n",
    "`.tokenize`. It does not yield an `~token.ENCODING` token.\n",
    "\n",
    "All constants from the `token` module are also exported from `tokenize`.\n",
    "\n",
    "Another function is provided to reverse the tokenization process. This\n",
    "is useful for creating tools that tokenize a script, modify the token\n",
    "stream, and write back the modified script.\n",
    "\n",
    "untokenize(iterable)\n",
    "\n",
    "Converts tokens back into Python source code. The *iterable* must return\n",
    "sequences with at least two elements, the token type and the token\n",
    "string. Any additional sequence elements are ignored.\n",
    "\n",
    "The reconstructed script is returned as a single string. The result is\n",
    "guaranteed to tokenize back to match the input so that the conversion is\n",
    "lossless and round-trips are assured. The guarantee applies only to the\n",
    "token type and token string as the spacing between tokens (column\n",
    "positions) may change.\n",
    "\n",
    "It returns bytes, encoded using the `~token.ENCODING` token, which is\n",
    "the first token sequence output by `.tokenize`. If there is no encoding\n",
    "token in the input, it returns a str instead.\n",
    "\n",
    "`.tokenize` needs to detect the encoding of source files it tokenizes.\n",
    "The function it uses to do this is available:\n",
    "\n",
    "detect_encoding(readline)\n",
    "\n",
    "The `detect_encoding` function is used to detect the encoding that\n",
    "should be used to decode a Python source file. It requires one argument,\n",
    "readline, in the same way as the `.tokenize` generator.\n",
    "\n",
    "It will call readline a maximum of twice, and return the encoding used\n",
    "(as a string) and a list of any lines (not decoded from bytes) it has\n",
    "read in.\n",
    "\n",
    "It detects the encoding from the presence of a UTF-8 BOM or an encoding\n",
    "cookie as specified in `263`. If both a BOM and a cookie are present,\n",
    "but disagree, a `SyntaxError` will be raised. Note that if the BOM is\n",
    "found, `'utf-8-sig'` will be returned as an encoding.\n",
    "\n",
    "If no encoding is specified, then the default of `'utf-8'` will be\n",
    "returned.\n",
    "\n",
    "Use `.open` to open Python source files: it uses `detect_encoding` to\n",
    "detect the file encoding.\n",
    "\n",
    "open(filename)\n",
    "\n",
    "Open a file in read only mode using the encoding detected by\n",
    "`detect_encoding`.\n",
    "\n",
    "3.2\n",
    "\n",
    "TokenError\n",
    "\n",
    "Raised when either a docstring or expression that may be split over\n",
    "several lines is not completed anywhere in the file, for example:\n",
    "\n",
    "    \"\"\"Beginning of\n",
    "    docstring\n",
    "\n",
    "or:\n",
    "\n",
    "    [1,\n",
    "     2,\n",
    "     3\n",
    "\n",
    "Note that unclosed single-quoted strings do not cause an error to be\n",
    "raised. They are tokenized as `~token.ERRORTOKEN`, followed by the\n",
    "tokenization of their contents.\n",
    "\n",
    "## Command-Line Usage\n",
    "\n",
    "3.3\n",
    "\n",
    "The `tokenize` module can be executed as a script from the command line.\n",
    "It is as simple as:\n",
    "\n",
    "``` sh\n",
    "python -m tokenize [-e] [filename.py]\n",
    "```\n",
    "\n",
    "The following options are accepted:\n",
    "\n",
    "tokenize\n",
    "\n",
    "-h, --help\n",
    "\n",
    "show this help message and exit\n",
    "\n",
    "-e, --exact\n",
    "\n",
    "display token names using the exact type\n",
    "\n",
    "If `filename.py` is specified its contents are tokenized to stdout.\n",
    "Otherwise, tokenization is performed on stdin.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Example of a script rewriter that transforms float literals into Decimal\n",
    "objects:\n",
    "\n",
    "    from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "    from io import BytesIO\n",
    "\n",
    "    def decistmt(s):\n",
    "        \"\"\"Substitute Decimals for floats in a string of statements.\n",
    "\n",
    "        >>> from decimal import Decimal\n",
    "        >>> s = 'print(+21.3e-5*-.1234/81.7)'\n",
    "        >>> decistmt(s)\n",
    "        \"print (+Decimal ('21.3e-5')*-Decimal ('.1234')/Decimal ('81.7'))\"\n",
    "\n",
    "        The format of the exponent is inherited from the platform C library.\n",
    "        Known cases are \"e-007\" (Windows) and \"e-07\" (not Windows).  Since\n",
    "        we're only showing 12 digits, and the 13th isn't close to 5, the\n",
    "        rest of the output should be platform-independent.\n",
    "\n",
    "        >>> exec(s)  #doctest: +ELLIPSIS\n",
    "        -3.21716034272e-0...7\n",
    "\n",
    "        Output from calculations with Decimal should be identical across all\n",
    "        platforms.\n",
    "\n",
    "        >>> exec(decistmt(s))\n",
    "        -3.217160342717258261933904529E-7\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        g = tokenize(BytesIO(s.encode('utf-8')).readline)  # tokenize the string\n",
    "        for toknum, tokval, _, _, _ in g:\n",
    "            if toknum == NUMBER and '.' in tokval:  # replace NUMBER tokens\n",
    "                result.extend([\n",
    "                    (NAME, 'Decimal'),\n",
    "                    (OP, '('),\n",
    "                    (STRING, repr(tokval)),\n",
    "                    (OP, ')')\n",
    "                ])\n",
    "            else:\n",
    "                result.append((toknum, tokval))\n",
    "        return untokenize(result).decode('utf-8')\n",
    "\n",
    "Example of tokenizing from the command line. The script:\n",
    "\n",
    "    def say_hello():\n",
    "        print(\"Hello, World!\")\n",
    "\n",
    "    say_hello()\n",
    "\n",
    "will be tokenized to the following output where the first column is the\n",
    "range of the line/column coordinates where the token is found, the\n",
    "second column is the name of the token, and the final column is the\n",
    "value of the token (if any)\n",
    "\n",
    "``` shell-session\n",
    "$ python -m tokenize hello.py\n",
    "0,0-0,0:            ENCODING       'utf-8'\n",
    "1,0-1,3:            NAME           'def'\n",
    "1,4-1,13:           NAME           'say_hello'\n",
    "1,13-1,14:          OP             '('\n",
    "1,14-1,15:          OP             ')'\n",
    "1,15-1,16:          OP             ':'\n",
    "1,16-1,17:          NEWLINE        '\\n'\n",
    "2,0-2,4:            INDENT         '    '\n",
    "2,4-2,9:            NAME           'print'\n",
    "2,9-2,10:           OP             '('\n",
    "2,10-2,25:          STRING         '\"Hello, World!\"'\n",
    "2,25-2,26:          OP             ')'\n",
    "2,26-2,27:          NEWLINE        '\\n'\n",
    "3,0-3,1:            NL             '\\n'\n",
    "4,0-4,0:            DEDENT         ''\n",
    "4,0-4,9:            NAME           'say_hello'\n",
    "4,9-4,10:           OP             '('\n",
    "4,10-4,11:          OP             ')'\n",
    "4,11-4,12:          NEWLINE        '\\n'\n",
    "5,0-5,0:            ENDMARKER      ''\n",
    "```\n",
    "\n",
    "The exact token type names can be displayed using the `-e` option:\n",
    "\n",
    "``` shell-session\n",
    "$ python -m tokenize -e hello.py\n",
    "0,0-0,0:            ENCODING       'utf-8'\n",
    "1,0-1,3:            NAME           'def'\n",
    "1,4-1,13:           NAME           'say_hello'\n",
    "1,13-1,14:          LPAR           '('\n",
    "1,14-1,15:          RPAR           ')'\n",
    "1,15-1,16:          COLON          ':'\n",
    "1,16-1,17:          NEWLINE        '\\n'\n",
    "2,0-2,4:            INDENT         '    '\n",
    "2,4-2,9:            NAME           'print'\n",
    "2,9-2,10:           LPAR           '('\n",
    "2,10-2,25:          STRING         '\"Hello, World!\"'\n",
    "2,25-2,26:          RPAR           ')'\n",
    "2,26-2,27:          NEWLINE        '\\n'\n",
    "3,0-3,1:            NL             '\\n'\n",
    "4,0-4,0:            DEDENT         ''\n",
    "4,0-4,9:            NAME           'say_hello'\n",
    "4,9-4,10:           LPAR           '('\n",
    "4,10-4,11:          RPAR           ')'\n",
    "4,11-4,12:          NEWLINE        '\\n'\n",
    "5,0-5,0:            ENDMARKER      ''\n",
    "```\n",
    "\n",
    "Example of tokenizing a file programmatically, reading unicode strings\n",
    "instead of bytes with `generate_tokens`:\n",
    "\n",
    "    import tokenize\n",
    "\n",
    "    with tokenize.open('hello.py') as f:\n",
    "        tokens = tokenize.generate_tokens(f.readline)\n",
    "        for token in tokens:\n",
    "            print(token)\n",
    "\n",
    "Or reading bytes directly with `.tokenize`:\n",
    "\n",
    "    import tokenize\n",
    "\n",
    "    with open('hello.py', 'rb') as f:\n",
    "        tokens = tokenize.tokenize(f.readline)\n",
    "        for token in tokens:\n",
    "            print(token)"
   ],
   "id": "bca560d9-f250-42e4-9909-8e07c20fdd1d"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
